{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO1TcWllr85J7IOu6IJXBJg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YD7TOD-dtVxu","executionInfo":{"status":"ok","timestamp":1750921009749,"user_tz":-330,"elapsed":179230,"user":{"displayName":"korupolu sujana","userId":"11553913231016006169"}},"outputId":"703bb3bf-de6f-4d75-e304-c4a91549b3cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”§ Setting up Citizen AI Platform environment...\n","âš¡ Using T4 GPU runtime\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hğŸ¯ CUDA Available: True\n","ğŸš€ GPU Device: Tesla T4\n","ğŸ’¾ GPU Memory: 15.8 GB\n","âœ… Environment setup complete!\n"]}],"source":["# CELL 1: Environment Setup and Package Installation\n","print(\"ğŸ”§ Setting up Citizen AI Platform environment...\")\n","print(\"âš¡ Using T4 GPU runtime\")\n","\n","# Install required packages\n","!pip install streamlit --quiet\n","!pip install transformers torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet\n","!pip install accelerate --quiet\n","!pip install plotly --quiet\n","!pip install textblob --quiet\n","!pip install wordcloud --quiet\n","!pip install matplotlib seaborn --quiet\n","!pip install pandas numpy --quiet\n","!pip install pyngrok --quiet\n","\n","# Download NLTK data for TextBlob\n","import nltk\n","nltk.download('punkt', quiet=True)\n","nltk.download('brown', quiet=True)\n","\n","# Verify GPU availability\n","import torch\n","print(f\"ğŸ¯ CUDA Available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"ğŸš€ GPU Device: {torch.cuda.get_device_name(0)}\")\n","    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n","\n","print(\"âœ… Environment setup complete!\")"]},{"cell_type":"code","source":["# CELL 2: Create Main Application File\n","app_code = '''\n","# Citizen AI - Intelligent Citizen Engagement Platform (Streamlit Version)\n","# Complete implementation with Granite 3.3 2B model and advanced features\n","\n","import streamlit as st\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime, timedelta\n","import json\n","import re\n","from collections import defaultdict, Counter\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from textblob import TextBlob\n","import time\n","\n","# Install required packages (run this in Google Colab first)\n","\"\"\"\n","!pip install streamlit transformers torch plotly textblob pandas numpy\n","!pip install accelerate bitsandbytes optimum\n","\n","# For even faster inference, also install:\n","!pip install flash-attn --no-build-isolation  # Optional for CUDA\n","\n","# To run: streamlit run citizen_ai_streamlit.py\n","\"\"\"\n","\n","# Page configuration\n","st.set_page_config(\n","    page_title=\"Citizen AI - Intelligent Engagement Platform\",\n","    page_icon=\"ğŸ›ï¸\",\n","    layout=\"wide\",\n","    initial_sidebar_state=\"expanded\"\n",")\n","\n","# Custom CSS for better styling\n","st.markdown(\"\"\"\n","<style>\n","    .main-header {\n","        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n","        padding: 1rem;\n","        border-radius: 10px;\n","        color: white;\n","        text-align: center;\n","        margin-bottom: 2rem;\n","    }\n","    .metric-card {\n","        background: #f8f9fa;\n","        padding: 1rem;\n","        border-radius: 10px;\n","        border-left: 4px solid #667eea;\n","        margin: 0.5rem 0;\n","    }\n","    .chat-message {\n","        padding: 1rem;\n","        border-radius: 10px;\n","        margin: 0.5rem 0;\n","    }\n","    .user-message {\n","        background: #e3f2fd;\n","        border-left: 4px solid #2196f3;\n","    }\n","    .assistant-message {\n","        background: #f3e5f5;\n","        border-left: 4px solid #9c27b0;\n","    }\n","    .sidebar-info {\n","        background: #fff3e0;\n","        padding: 1rem;\n","        border-radius: 10px;\n","        border: 1px solid #ff9800;\n","    }\n","</style>\n","\"\"\", unsafe_allow_html=True)\n","\n","class CitizenAI:\n","    def __init__(self):\n","      self.model_name = \"ibm-granite/granite-3.3-2b-instruct\"\n","      #self.model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n","\n","\n","    @st.cache_resource\n","    def load_model(_self):\n","        \"\"\"Load the Granite 3.3 2B model with caching\"\"\"\n","        try:\n","            with st.spinner(\"ğŸ”„ Loading Granite 3.3 2B model... This may take a few minutes.\"):\n","                progress_bar = st.progress(0)\n","\n","                # Load tokenizer\n","                progress_bar.progress(25)\n","                tokenizer = AutoTokenizer.from_pretrained(\n","                    _self.model_name,\n","                    use_fast=True\n","                )\n","                #tokenizer = AutoTokenizer.from_pretrained(_self.model_name)\n","\n","                # Check device\n","                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","                # Load model\n","                progress_bar.progress(50)\n","                model = AutoModelForCausalLM.from_pretrained(\n","                    _self.model_name,\n","                    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n","                    device_map=\"auto\" if device == \"cuda\" else None,\n","                    low_cpu_mem_usage=True,\n","                    use_cache=True\n","                )\n","                # progress_bar.progress(50)\n","                # model = AutoModelForCausalLM.from_pretrained(\n","                #   _self.model_name,\n","                #   torch_dtype=torch.float16,\n","                #   device_map=\"auto\"\n","                # )\n","\n","                # Move to device\n","                progress_bar.progress(75)\n","                if device == \"cuda\":\n","                    model = model.to(device)\n","\n","                # Set pad token\n","                if tokenizer.pad_token is None:\n","                    tokenizer.pad_token = tokenizer.eos_token\n","\n","                # Compile model for speed\n","                try:\n","                    model = torch.compile(model, mode=\"reduce-overhead\")\n","                except:\n","                    pass\n","\n","                progress_bar.progress(100)\n","                st.success(\"âœ… Model loaded successfully!\")\n","\n","                return tokenizer, model, device\n","\n","        except Exception as e:\n","            st.error(f\"âŒ Error loading model: {str(e)}\")\n","            return None, None, None\n","\n","    def analyze_sentiment(self, text):\n","        \"\"\"Analyze sentiment of citizen input\"\"\"\n","        try:\n","            blob = TextBlob(text)\n","            sentiment_score = blob.sentiment.polarity\n","\n","            if sentiment_score > 0.1:\n","                sentiment_label = \"Positive\"\n","                emoji = \"ğŸ˜Š\"\n","            elif sentiment_score < -0.1:\n","                sentiment_label = \"Negative\"\n","                emoji = \"ğŸ˜Ÿ\"\n","            else:\n","                sentiment_label = \"Neutral\"\n","                emoji = \"ğŸ˜\"\n","\n","            return {\n","                'score': sentiment_score,\n","                'label': sentiment_label,\n","                'emoji': emoji,\n","                'confidence': abs(sentiment_score)\n","            }\n","        except:\n","            return {\n","                'score': 0.0,\n","                'label': \"Neutral\",\n","                'emoji': \"ğŸ˜\",\n","                'confidence': 0.0\n","            }\n","\n","    def extract_topics(self, text):\n","        \"\"\"Extract key topics from citizen input\"\"\"\n","        topic_keywords = {\n","            'infrastructure': ['road', 'bridge', 'water', 'electricity', 'internet', 'transport'],\n","            'healthcare': ['hospital', 'doctor', 'medicine', 'health', 'clinic', 'emergency'],\n","            'education': ['school', 'college', 'teacher', 'student', 'education', 'learning'],\n","            'environment': ['pollution', 'waste', 'climate', 'green', 'clean', 'environment'],\n","            'safety': ['police', 'crime', 'safety', 'security', 'violence', 'emergency'],\n","            'economy': ['job', 'employment', 'business', 'economy', 'money', 'tax'],\n","            'governance': ['government', 'policy', 'law', 'regulation', 'administration', 'service']\n","        }\n","\n","        text_lower = text.lower()\n","        detected_topics = []\n","\n","        for topic, keywords in topic_keywords.items():\n","            if any(keyword in text_lower for keyword in keywords):\n","                detected_topics.append(topic)\n","\n","        return detected_topics if detected_topics else ['general']\n","\n","# Initialize session state\n","def initialize_session_state():\n","    if 'conversation_history' not in st.session_state:\n","        st.session_state.conversation_history = []\n","    if 'sentiment_data' not in st.session_state:\n","        st.session_state.sentiment_data = []\n","    if 'topics_discussed' not in st.session_state:\n","        st.session_state.topics_discussed = defaultdict(int)\n","    if 'model_loaded' not in st.session_state:\n","        st.session_state.model_loaded = False\n","    if 'tokenizer' not in st.session_state:\n","        st.session_state.tokenizer = None\n","    if 'model' not in st.session_state:\n","        st.session_state.model = None\n","    if 'device' not in st.session_state:\n","        st.session_state.device = None\n","\n","def generate_response(user_input, user_name=\"Citizen\"):\n","    \"\"\"Generate response using the loaded model\"\"\"\n","    if not st.session_state.model_loaded:\n","        return \"Please load the model first using the sidebar.\"\n","\n","    try:\n","        ai = CitizenAI()\n","\n","        # Analyze input\n","        sentiment = ai.analyze_sentiment(user_input)\n","        topics = ai.extract_topics(user_input)\n","\n","        # Update topics counter\n","        for topic in topics:\n","            st.session_state.topics_discussed[topic] += 1\n","\n","        # Create prompt\n","        system_prompt = f\"\"\"You are a helpful AI assistant for citizen engagement. Help citizens with their concerns about government services and community issues.\n","\n","Citizen: {user_name}\n","Topics: {', '.join(topics)}\n","Sentiment: {sentiment['label']}\n","\n","Provide a helpful, concise response (max 2-3 sentences).\"\"\"\n","\n","        # Get context from recent history\n","        context_msg = \"\"\n","        if st.session_state.conversation_history:\n","            last_exchange = st.session_state.conversation_history[-1]\n","            context_msg = f\"Previous: {last_exchange['user'][:50]}...\"\n","\n","        full_prompt = f\"\"\"{system_prompt}\\n\\nContext: {context_msg}\\n\\nCitizen: {user_input}\\n\\nAssistant:\"\"\"\n","\n","        # Tokenize\n","        inputs = st.session_state.tokenizer(\n","            full_prompt,\n","            return_tensors=\"pt\",\n","            truncation=True,\n","            max_length=512,\n","            padding=False\n","        )\n","\n","        # Move to device\n","        inputs = {k: v.to(st.session_state.device) for k, v in inputs.items()}\n","\n","        # Generate\n","        with torch.no_grad():\n","            outputs = st.session_state.model.generate(\n","                **inputs,\n","                max_new_tokens=100,\n","                temperature=0.3,\n","                do_sample=True,\n","                top_p=0.9,\n","                repetition_penalty=1.1,\n","                pad_token_id=st.session_state.tokenizer.eos_token_id,\n","                eos_token_id=st.session_state.tokenizer.eos_token_id,\n","                use_cache=True\n","            )\n","\n","        # Decode\n","        response = st.session_state.tokenizer.decode(\n","            outputs[0][inputs['input_ids'].shape[1]:],\n","            skip_special_tokens=True\n","        ).strip()\n","\n","        # Fallback if empty\n","        if not response:\n","            response = f\"Thank you for your concern about {topics[0] if topics else 'this matter'}. I understand your {sentiment['label'].lower()} feelings. How can I help you further?\"\n","\n","        # Store conversation\n","        conversation_entry = {\n","            'timestamp': datetime.now(),\n","            'user': user_input,\n","            'assistant': response,\n","            'sentiment': sentiment,\n","            'topics': topics,\n","            'user_name': user_name\n","        }\n","\n","        st.session_state.conversation_history.append(conversation_entry)\n","\n","        # Store sentiment data\n","        st.session_state.sentiment_data.append({\n","            'timestamp': datetime.now(),\n","            'sentiment_score': sentiment['score'],\n","            'sentiment_label': sentiment['label'],\n","            'topics': topics\n","        })\n","\n","        # Limit history\n","        if len(st.session_state.conversation_history) > 20:\n","            st.session_state.conversation_history = st.session_state.conversation_history[-20:]\n","\n","        if len(st.session_state.sentiment_data) > 50:\n","            st.session_state.sentiment_data = st.session_state.sentiment_data[-50:]\n","\n","        return response\n","\n","    except Exception as e:\n","        st.error(f\"Error generating response: {str(e)}\")\n","        return f\"I understand your concern. Could you provide more specific details about what you need assistance with?\"\n","\n","def create_sentiment_chart():\n","    \"\"\"Create sentiment analysis chart\"\"\"\n","    if not st.session_state.sentiment_data:\n","        fig = go.Figure()\n","        fig.add_annotation(text=\"No sentiment data available yet\", x=0.5, y=0.5, showarrow=False)\n","        fig.update_layout(title=\"Sentiment Analysis Over Time\", height=400)\n","        return fig\n","\n","    df = pd.DataFrame(st.session_state.sentiment_data)\n","    df['timestamp'] = pd.to_datetime(df['timestamp'])\n","\n","    fig = go.Figure()\n","    fig.add_trace(go.Scatter(\n","        x=df['timestamp'],\n","        y=df['sentiment_score'],\n","        mode='lines+markers',\n","        name='Sentiment Score',\n","        line=dict(color='#667eea', width=3),\n","        marker=dict(size=8, color='#764ba2')\n","    ))\n","\n","    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"Neutral\")\n","    fig.update_layout(\n","        title=\"ğŸ“ˆ Citizen Sentiment Over Time\",\n","        xaxis_title=\"Time\",\n","        yaxis_title=\"Sentiment Score\",\n","        yaxis=dict(range=[-1, 1]),\n","        height=400,\n","        plot_bgcolor='rgba(0,0,0,0)',\n","        paper_bgcolor='rgba(0,0,0,0)'\n","    )\n","\n","    return fig\n","\n","def create_topics_chart():\n","    \"\"\"Create topics distribution chart\"\"\"\n","    if not st.session_state.topics_discussed:\n","        fig = go.Figure()\n","        fig.add_annotation(text=\"No topics discussed yet\", x=0.5, y=0.5, showarrow=False)\n","        fig.update_layout(title=\"Topics Distribution\", height=400)\n","        return fig\n","\n","    topics = list(st.session_state.topics_discussed.keys())\n","    counts = list(st.session_state.topics_discussed.values())\n","\n","    fig = go.Figure(data=[go.Bar(\n","        x=topics,\n","        y=counts,\n","        marker_color='#667eea',\n","        text=counts,\n","        textposition='auto',\n","    )])\n","\n","    fig.update_layout(\n","        title=\"ğŸ·ï¸ Most Discussed Topics\",\n","        xaxis_title=\"Topics\",\n","        yaxis_title=\"Frequency\",\n","        height=400,\n","        plot_bgcolor='rgba(0,0,0,0)',\n","        paper_bgcolor='rgba(0,0,0,0)'\n","    )\n","\n","    return fig\n","\n","def main():\n","    # Initialize session state\n","    initialize_session_state()\n","\n","    # Header\n","    st.markdown(\"\"\"\n","    <div class=\"main-header\">\n","        <h1>ğŸ›ï¸ Citizen AI - Intelligent Citizen Engagement Platform</h1>\n","        <p>Empowering citizens through AI-powered conversations and insights</p>\n","    </div>\n","    \"\"\", unsafe_allow_html=True)\n","\n","    # Sidebar\n","    with st.sidebar:\n","        st.markdown(\"## ğŸ”§ Model Management\")\n","\n","        if st.button(\"ğŸš€ Initialize Model\", type=\"primary\", use_container_width=True):\n","            ai = CitizenAI()\n","            tokenizer, model, device = ai.load_model()\n","\n","            if tokenizer and model:\n","                st.session_state.tokenizer = tokenizer\n","                st.session_state.model = model\n","                st.session_state.device = device\n","                st.session_state.model_loaded = True\n","                st.rerun()\n","\n","        # Model status\n","        if st.session_state.model_loaded:\n","            st.success(\"âœ… Model Ready\")\n","            device_info = st.session_state.device if st.session_state.device else \"CPU\"\n","            st.info(f\"ğŸ–¥ï¸ Device: {device_info}\")\n","        else:\n","            st.warning(\"âš ï¸ Model not loaded\")\n","\n","        st.markdown(\"---\")\n","\n","        # Quick stats\n","        st.markdown(\"## ğŸ“Š Quick Stats\")\n","        total_conversations = len(st.session_state.conversation_history)\n","\n","        if st.session_state.sentiment_data:\n","            avg_sentiment = np.mean([s['sentiment_score'] for s in st.session_state.sentiment_data])\n","            if avg_sentiment > 0.1:\n","                sentiment_emoji = \"ğŸ˜Š Positive\"\n","            elif avg_sentiment < -0.1:\n","                sentiment_emoji = \"ğŸ˜Ÿ Negative\"\n","            else:\n","                sentiment_emoji = \"ğŸ˜ Neutral\"\n","        else:\n","            avg_sentiment = 0.0\n","            sentiment_emoji = \"ğŸ˜ Neutral\"\n","\n","        st.metric(\"Total Conversations\", total_conversations)\n","        st.metric(\"Average Sentiment\", f\"{avg_sentiment:.2f}\")\n","        st.markdown(f\"**Mood**: {sentiment_emoji}\")\n","\n","        st.markdown(\"---\")\n","\n","        if st.button(\"ğŸ—‘ï¸ Clear All Data\", type=\"secondary\", use_container_width=True):\n","            st.session_state.conversation_history = []\n","            st.session_state.sentiment_data = []\n","            st.session_state.topics_discussed = defaultdict(int)\n","            st.success(\"Data cleared!\")\n","            st.rerun()\n","\n","        # Info section\n","        st.markdown(\"\"\"\n","        <div class=\"sidebar-info\">\n","        <strong>ğŸ’¡ Tips:</strong><br>\n","        â€¢ Initialize the model first<br>\n","        â€¢ Ask about government services<br>\n","        â€¢ Share your concerns<br>\n","        â€¢ Check the dashboard for insights\n","        </div>\n","        \"\"\", unsafe_allow_html=True)\n","\n","    # Main content tabs\n","    tab1, tab2, tab3 = st.tabs([\"ğŸ’¬ Chat\", \"ğŸ“Š Dashboard\", \"ğŸ“ˆ Analytics\"])\n","\n","    with tab1:\n","        st.markdown(\"## ğŸ’¬ Chat with Citizen AI\")\n","\n","        # User input section\n","        col1, col2 = st.columns([3, 1])\n","\n","        with col1:\n","            user_name = st.text_input(\"ğŸ‘¤ Your Name:\", value=\"Citizen\", key=\"user_name\")\n","\n","        with col2:\n","            if st.session_state.model_loaded:\n","                st.success(\"ğŸŸ¢ Ready\")\n","            else:\n","                st.error(\"ğŸ”´ Load Model\")\n","\n","        # Chat input\n","        user_input = st.text_area(\n","            \"ğŸ’­ Your Message:\",\n","            placeholder=\"Ask about government services, policies, or share your concerns...\",\n","            height=100,\n","            key=\"chat_input\"\n","        )\n","\n","        col1, col2, col3 = st.columns([1, 1, 2])\n","\n","        with col1:\n","            send_button = st.button(\"ğŸ“¤ Send\", type=\"primary\", use_container_width=True)\n","\n","        with col2:\n","            clear_chat = st.button(\"ğŸ§¹ Clear Chat\", use_container_width=True)\n","\n","        # Process input\n","        if send_button and user_input.strip():\n","            if st.session_state.model_loaded:\n","                with st.spinner(\"ğŸ¤” Thinking...\"):\n","                    response = generate_response(user_input, user_name)\n","                st.rerun()\n","            else:\n","                st.error(\"Please initialize the model first!\")\n","\n","        if clear_chat:\n","            st.session_state.conversation_history = []\n","            st.rerun()\n","\n","        # Display conversation\n","        st.markdown(\"### ğŸ’¬ Conversation\")\n","\n","        if st.session_state.conversation_history:\n","            for entry in reversed(st.session_state.conversation_history[-10:]):  # Show last 10\n","                # User message\n","                st.markdown(f\"\"\"\n","                <div class=\"chat-message user-message\">\n","                    <strong>ğŸ‘¤ {entry['user_name']}:</strong><br>\n","                    {entry['user']}\n","                    <br><small>ğŸ•’ {entry['timestamp'].strftime('%H:%M:%S')} |\n","                    {entry['sentiment']['emoji']} {entry['sentiment']['label']} |\n","                    ğŸ·ï¸ {', '.join(entry['topics'])}</small>\n","                </div>\n","                \"\"\", unsafe_allow_html=True)\n","\n","                # Assistant message\n","                st.markdown(f\"\"\"\n","                <div class=\"chat-message assistant-message\">\n","                    <strong>ğŸ¤– Citizen AI:</strong><br>\n","                    {entry['assistant']}\n","                </div>\n","                \"\"\", unsafe_allow_html=True)\n","        else:\n","            st.info(\"ğŸ‘‹ Start a conversation by sending a message!\")\n","\n","    with tab2:\n","        st.markdown(\"## ğŸ“Š Real-Time Dashboard\")\n","\n","        # Key metrics\n","        col1, col2, col3, col4 = st.columns(4)\n","\n","        with col1:\n","            st.metric(\n","                \"ğŸ“± Total Conversations\",\n","                len(st.session_state.conversation_history)\n","            )\n","\n","        with col2:\n","            if st.session_state.sentiment_data:\n","                avg_sentiment = np.mean([s['sentiment_score'] for s in st.session_state.sentiment_data])\n","                st.metric(\n","                    \"ğŸ˜Š Average Sentiment\",\n","                    f\"{avg_sentiment:.2f}\"\n","                )\n","            else:\n","                st.metric(\"ğŸ˜Š Average Sentiment\", \"0.00\")\n","\n","        with col3:\n","            active_topics = len(st.session_state.topics_discussed)\n","            st.metric(\n","                \"ğŸ·ï¸ Active Topics\",\n","                active_topics\n","            )\n","\n","        with col4:\n","            recent_conversations = len([\n","                c for c in st.session_state.conversation_history\n","                if c['timestamp'] > datetime.now() - timedelta(hours=1)\n","            ])\n","            engagement_score = min(recent_conversations * 10, 100)\n","            st.metric(\n","                \"ğŸ”¥ Engagement Score\",\n","                f\"{engagement_score}/100\"\n","            )\n","\n","        # Sentiment distribution\n","        if st.session_state.sentiment_data:\n","            st.markdown(\"### ğŸ“Š Sentiment Distribution\")\n","\n","            sentiment_counts = Counter([s['sentiment_label'] for s in st.session_state.sentiment_data])\n","\n","            col1, col2, col3 = st.columns(3)\n","\n","            with col1:\n","                st.metric(\"ğŸ˜Š Positive\", sentiment_counts.get('Positive', 0))\n","            with col2:\n","                st.metric(\"ğŸ˜ Neutral\", sentiment_counts.get('Neutral', 0))\n","            with col3:\n","                st.metric(\"ğŸ˜Ÿ Negative\", sentiment_counts.get('Negative', 0))\n","\n","        # Top concerns\n","        if st.session_state.topics_discussed:\n","            st.markdown(\"### ğŸ” Top Citizen Concerns\")\n","\n","            top_topics = Counter(st.session_state.topics_discussed).most_common(5)\n","\n","            for i, (topic, count) in enumerate(top_topics, 1):\n","                st.markdown(f\"**{i}. {topic.title()}**: {count} mentions\")\n","\n","    with tab3:\n","        st.markdown(\"## ğŸ“ˆ Advanced Analytics\")\n","\n","        col1, col2 = st.columns(2)\n","\n","        with col1:\n","            # Sentiment chart\n","            sentiment_fig = create_sentiment_chart()\n","            st.plotly_chart(sentiment_fig, use_container_width=True)\n","\n","        with col2:\n","            # Topics chart\n","            topics_fig = create_topics_chart()\n","            st.plotly_chart(topics_fig, use_container_width=True)\n","\n","        # Detailed data table\n","        if st.session_state.conversation_history:\n","            st.markdown(\"### ğŸ“‹ Conversation Details\")\n","\n","            # Create DataFrame\n","            data = []\n","            for entry in st.session_state.conversation_history:\n","                data.append({\n","                    'Time': entry['timestamp'].strftime('%Y-%m-%d %H:%M:%S'),\n","                    'Citizen': entry['user_name'],\n","                    'Message Length': len(entry['user']),\n","                    'Sentiment': entry['sentiment']['label'],\n","                    'Score': round(entry['sentiment']['score'], 3),\n","                    'Topics': ', '.join(entry['topics'])\n","                })\n","\n","            df = pd.DataFrame(data)\n","            st.dataframe(df, use_container_width=True)\n","\n","            # Export option\n","            csv = df.to_csv(index=False)\n","            st.download_button(\n","                label=\"ğŸ“¥ Download Conversation Data\",\n","                data=csv,\n","                file_name=f\"citizen_ai_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n","                mime=\"text/csv\"\n","            )\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n","# Run Streamlit\n","!streamlit run citizen_ai_streamlit.py #&\n","# import subprocess\n","# subprocess.run([\"streamlit\", \"run\", \"citizen_ai_streamlit.py\"])\n","\n","\n","\n","# # Create public tunnel\n","# from pyngrok import ngrok\n","# public_url = ngrok.connect(8501)\n","# print(f\"Public URL: {public_url}\")\n","'''\n","\n","# Write the app code to a file\n","with open('citizen_ai_app.py', 'w', encoding='utf-8') as f:\n","    f.write(app_code)\n","\n","print(\"âœ… Main application file 'citizen_ai_app.py' created successfully!\")\n","print(\"ğŸ“ File size:\", len(app_code), \"characters\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BpfPOuaYuF4U","executionInfo":{"status":"ok","timestamp":1750923948560,"user_tz":-330,"elapsed":71,"user":{"displayName":"korupolu sujana","userId":"11553913231016006169"}},"outputId":"e844beb6-e83a-4821-b40a-a6a351f4c693"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Main application file 'citizen_ai_app.py' created successfully!\n","ğŸ“ File size: 21695 characters\n"]}]},{"cell_type":"code","source":["# CELL 3: Setup ngrok for public access\n","#!pip install pyngrok\n","from pyngrok import ngrok\n","import os\n","\n","# IMPORTANT: Replace 'YOUR_NGROK_TOKEN_HERE' with your actual token from ngrok.com\n","NGROK_TOKEN = \"2yrFE9nfLipk5px1qItu6xTbLTR_3F18YPf9j15cDSNq7zR1a\"  # ğŸ‘ˆ PASTE YOUR TOKEN HERE\n","\n","# Set authentication token\n","try:\n","    ngrok.set_auth_token(NGROK_TOKEN)\n","    print(\"âœ… ngrok authentication token set successfully!\")\n","except Exception as e:\n","    print(f\"âŒ Error setting ngrok token: {e}\")\n","    print(\"ğŸ”‘ Please make sure you've replaced 'YOUR_NGROK_TOKEN_HERE' with your actual token\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yovv2GpSvNtF","executionInfo":{"status":"ok","timestamp":1750923949519,"user_tz":-330,"elapsed":7,"user":{"displayName":"korupolu sujana","userId":"11553913231016006169"}},"outputId":"a66a0631-10f5-4e37-c6b0-a4b6381e9233"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… ngrok authentication token set successfully!\n"]}]},{"cell_type":"code","source":["# FIXED DEPLOYMENT SCRIPT - Replace Cell 4 with this code\n","\n","import subprocess\n","import threading\n","import time\n","from pyngrok import ngrok\n","import sys\n","import os\n","\n","print(\"ğŸ”„ Stopping previous deployment...\")\n","ngrok.kill()\n","\n","print(\"ğŸ”§ Fixing Streamlit installation...\")\n","# Ensure Streamlit is properly installed and accessible\n","try:\n","    # Reinstall streamlit with explicit path\n","    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"streamlit\", \"--upgrade\", \"--force-reinstall\"],\n","                          capture_output=True, text=True, timeout=120)\n","    print(\"âœ… Streamlit installation completed\")\n","\n","    # Verify streamlit installation\n","    result = subprocess.run([sys.executable, \"-m\", \"streamlit\", \"--version\"],\n","                          capture_output=True, text=True, timeout=30)\n","    if result.returncode == 0:\n","        print(f\"âœ… Streamlit version: {result.stdout.strip()}\")\n","    else:\n","        print(f\"âš ï¸  Streamlit verification: {result.stderr}\")\n","\n","except Exception as e:\n","    print(f\"âŒ Error with Streamlit installation: {e}\")\n","\n","print(\"ğŸš€ Starting Citizen AI Platform...\")\n","\n","# Function to run Streamlit using Python module approach\n","def run_streamlit():\n","    \"\"\"Run Streamlit using python -m streamlit approach\"\"\"\n","    try:\n","        # Use python -m streamlit instead of direct streamlit command\n","        cmd = [\n","            sys.executable, \"-m\", \"streamlit\", \"run\", \"citizen_ai_app.py\",\n","            \"--server.port\", \"8501\",\n","            \"--server.headless\", \"true\",\n","            \"--server.fileWatcherType\", \"none\",\n","            \"--browser.gatherUsageStats\", \"false\",\n","            \"--server.enableCORS\", \"false\",\n","            \"--server.enableXsrfProtection\", \"false\"\n","        ]\n","\n","        print(f\"ğŸ”§ Running command: {' '.join(cmd)}\")\n","        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n","\n","        # Monitor the process\n","        for line in iter(process.stdout.readline, ''):\n","            if line.strip():\n","                print(f\"ğŸ“¡ Streamlit: {line.strip()}\")\n","            if \"Network URL\" in line or \"External URL\" in line:\n","                break\n","\n","    except Exception as e:\n","        print(f\"âŒ Error running Streamlit: {e}\")\n","\n","# Start Streamlit in background thread\n","print(\"ğŸ“¡ Starting Streamlit server...\")\n","streamlit_thread = threading.Thread(target=run_streamlit, daemon=True)\n","streamlit_thread.start()\n","\n","# Wait longer for Streamlit to fully start\n","print(\"â³ Waiting for Streamlit to initialize...\")\n","time.sleep(25)\n","\n","# Test if Streamlit is running by checking the port\n","import socket\n","def check_port(host, port):\n","    try:\n","        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n","        sock.settimeout(3)\n","        result = sock.connect_ex((host, port))\n","        sock.close()\n","        return result == 0\n","    except:\n","        return False\n","\n","# Check if Streamlit is running\n","if check_port('localhost', 8501):\n","    print(\"âœ… Streamlit server is responding on port 8501\")\n","else:\n","    print(\"âŒ Streamlit server not responding. Trying alternative approach...\")\n","\n","    # Alternative: Run streamlit directly with full path\n","    streamlit_path = subprocess.run([sys.executable, \"-c\", \"import streamlit; print(streamlit.__file__)\"],\n","                                  capture_output=True, text=True)\n","    if streamlit_path.returncode == 0:\n","        streamlit_dir = os.path.dirname(streamlit_path.stdout.strip())\n","        streamlit_script = os.path.join(streamlit_dir, \"web\", \"cli.py\")\n","\n","        # Try running streamlit CLI directly\n","        def run_streamlit_alt():\n","            try:\n","                subprocess.run([sys.executable, streamlit_script, \"run\", \"citizen_ai_app.py\",\n","                              \"--server.port\", \"8501\", \"--server.headless\", \"true\"])\n","            except Exception as e:\n","                print(f\"Alternative streamlit failed: {e}\")\n","\n","        alt_thread = threading.Thread(target=run_streamlit_alt, daemon=True)\n","        alt_thread.start()\n","        time.sleep(15)\n","\n","# Create ngrok tunnel\n","try:\n","    print(\"ğŸŒ Creating public tunnel...\")\n","\n","    # Create tunnel with retry logic\n","    max_retries = 3\n","    tunnel = None\n","\n","    for attempt in range(max_retries):\n","        try:\n","            tunnel = ngrok.connect(8501, proto=\"http\", bind_tls=True)\n","            break\n","        except Exception as e:\n","            print(f\"âŒ Tunnel attempt {attempt + 1} failed: {e}\")\n","            if attempt < max_retries - 1:\n","                time.sleep(5)\n","                continue\n","            else:\n","                raise e\n","\n","    if tunnel:\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"ğŸ‰ CITIZEN AI PLATFORM IS DEPLOYED!\")\n","        print(\"=\"*70)\n","        print(f\"ğŸŒ Public URL: {tunnel.public_url}\")\n","        print(f\"ğŸ”— Click here: {tunnel.public_url}\")\n","        print(\"=\"*70)\n","        print(\"\\nğŸ¯ Application Features:\")\n","        print(\"âœ… Real-Time AI Assistant (IBM Granite-3.3-2B)\")\n","        print(\"âœ… Citizen Sentiment Analysis\")\n","        print(\"âœ… Dynamic Dashboard & Analytics\")\n","        print(\"âœ… Personalized Contextual Responses\")\n","        print(\"âœ… T4 GPU Acceleration\")\n","        print(\"\\nğŸ’¡ First-Time Setup:\")\n","        print(\"â€¢ The AI model will load when you first visit (2-3 minutes)\")\n","        print(\"â€¢ Set your location and interests in the sidebar\")\n","        print(\"â€¢ Try asking: 'How do I register to vote?'\")\n","        print(\"\\nâš ï¸  IMPORTANT: Keep this cell running to maintain access!\")\n","        print(\"=\"*70)\n","\n","        # Keep tunnel alive with better error handling\n","        try:\n","            while True:\n","                time.sleep(30)\n","                # Check if tunnel is still active\n","                try:\n","                    tunnels = ngrok.get_tunnels()\n","                    if not tunnels:\n","                        print(\"ğŸ”„ Tunnel disconnected, recreating...\")\n","                        tunnel = ngrok.connect(8501, proto=\"http\", bind_tls=True)\n","                        print(f\"ğŸŒ New URL: {tunnel.public_url}\")\n","                    else:\n","                        print(f\"ğŸ”„ App running... Access: {tunnel.public_url}\")\n","                except Exception as tunnel_check_error:\n","                    print(f\"âš ï¸  Tunnel check error: {tunnel_check_error}\")\n","\n","        except KeyboardInterrupt:\n","            print(\"\\nğŸ›‘ Shutting down Citizen AI Platform...\")\n","            ngrok.kill()\n","\n","except Exception as e:\n","    print(f\"âŒ Error creating tunnel: {e}\")\n","    print(\"\\nğŸ”§ Troubleshooting Steps:\")\n","    print(\"1. Make sure your ngrok token is set correctly\")\n","    print(\"2. Try restarting the runtime: Runtime â†’ Restart and run all\")\n","    print(\"3. Check if all packages installed correctly\")\n","\n","    # Show alternative local access\n","    print(f\"\\nğŸ’¡ Alternative: If you're running locally, try: http://localhost:8501\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"4r1ZlSCA9mhH","outputId":"222b2319-0895-41c3-d349-feacb305252e","executionInfo":{"status":"error","timestamp":1750925393602,"user_tz":-330,"elapsed":52,"user":{"displayName":"korupolu sujana","userId":"11553913231016006169"}}},"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'pyngrok'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1-2881176486.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyngrok'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["# VERIFICATION SCRIPT - Run this first to diagnose issues\n","\n","import sys\n","import subprocess\n","import os\n","import importlib.util\n","\n","print(\"ğŸ” CITIZEN AI PLATFORM - DIAGNOSTIC CHECK\")\n","print(\"=\"*50)\n","\n","# Check Python version\n","print(f\"ğŸ Python Version: {sys.version}\")\n","\n","# Check if we're in Colab\n","try:\n","    import google.colab\n","    print(\"âœ… Running in Google Colab\")\n","    IN_COLAB = True\n","except ImportError:\n","    print(\"âŒ Not running in Google Colab\")\n","    IN_COLAB = False\n","\n","# Check GPU availability\n","try:\n","    import torch\n","    print(f\"ğŸš€ PyTorch Version: {torch.__version__}\")\n","    print(f\"ğŸ¯ CUDA Available: {torch.cuda.is_available()}\")\n","    if torch.cuda.is_available():\n","        print(f\"ğŸ”¥ GPU Device: {torch.cuda.get_device_name(0)}\")\n","        print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n","except Exception as e:\n","    print(f\"âŒ PyTorch/CUDA Error: {e}\")\n","\n","print(\"\\nğŸ”§ PACKAGE INSTALLATION CHECK\")\n","print(\"=\"*50)\n","\n","# List of required packages\n","required_packages = [\n","    'streamlit', 'transformers', 'torch', 'plotly',\n","    'textblob', 'wordcloud', 'matplotlib', 'seaborn',\n","    'pandas', 'numpy', 'pyngrok'\n","]\n","\n","missing_packages = []\n","for package in required_packages:\n","    try:\n","        spec = importlib.util.find_spec(package)\n","        if spec is not None:\n","            print(f\"âœ… {package}\")\n","        else:\n","            print(f\"âŒ {package} - NOT FOUND\")\n","            missing_packages.append(package)\n","    except Exception as e:\n","        print(f\"âŒ {package} - ERROR: {e}\")\n","        missing_packages.append(package)\n","\n","# Install missing packages\n","if missing_packages:\n","    print(f\"\\nğŸ”„ Installing missing packages: {missing_packages}\")\n","    for package in missing_packages:\n","        try:\n","            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package],\n","                         check=True, capture_output=True)\n","            print(f\"âœ… Installed {package}\")\n","        except Exception as e:\n","            print(f\"âŒ Failed to install {package}: {e}\")\n","\n","print(\"\\nğŸ“ FILE CHECK\")\n","print(\"=\"*50)\n","\n","# Check if main app file exists\n","if os.path.exists('citizen_ai_app.py'):\n","    print(\"âœ… citizen_ai_app.py exists\")\n","    file_size = os.path.getsize('citizen_ai_app.py')\n","    print(f\"ğŸ“ File size: {file_size} bytes\")\n","\n","    # Check if file is not empty and has expected content\n","    with open('citizen_ai_app.py', 'r') as f:\n","        content = f.read()\n","        if 'streamlit' in content and 'granite' in content:\n","            print(\"âœ… File contains expected content\")\n","        else:\n","            print(\"âŒ File may be corrupted or incomplete\")\n","else:\n","    print(\"âŒ citizen_ai_app.py NOT FOUND\")\n","    print(\"ğŸ”„ Creating the file now...\")\n","\n","    # Recreate the file if missing\n","    app_code = '''\n","import streamlit as st\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import plotly.express as px\n","import pandas as pd\n","import numpy as np\n","from textblob import TextBlob\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","import re\n","from collections import defaultdict, Counter\n","\n","st.set_page_config(\n","    page_title=\"Citizen AI - Test\",\n","    page_icon=\"ğŸ›ï¸\",\n","    layout=\"wide\"\n",")\n","\n","st.title(\"ğŸ›ï¸ Citizen AI - Platform Test\")\n","st.write(\"If you can see this, the basic setup is working!\")\n","\n","# Test basic functionality\n","st.header(\"ğŸ”§ System Check\")\n","\n","# Check GPU\n","if torch.cuda.is_available():\n","    st.success(f\"âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n","else:\n","    st.warning(\"âš ï¸ No GPU detected\")\n","\n","# Simple test\n","st.header(\"ğŸ’¬ Simple Test\")\n","user_input = st.text_input(\"Enter a test message:\")\n","if user_input:\n","    st.write(f\"You said: {user_input}\")\n","\n","    # Test sentiment analysis\n","    blob = TextBlob(user_input)\n","    sentiment = \"Positive\" if blob.sentiment.polarity > 0 else \"Negative\" if blob.sentiment.polarity < 0 else \"Neutral\"\n","    st.write(f\"Sentiment: {sentiment}\")\n","\n","st.success(\"ğŸ‰ Basic functionality test passed!\")\n","'''\n","\n","    with open('citizen_ai_app.py', 'w') as f:\n","        f.write(app_code)\n","    print(\"âœ… Created basic test version of citizen_ai_app.py\")\n","\n","print(\"\\nğŸŒ STREAMLIT CHECK\")\n","print(\"=\"*50)\n","\n","# Test Streamlit command\n","try:\n","    result = subprocess.run([sys.executable, \"-m\", \"streamlit\", \"--version\"],\n","                          capture_output=True, text=True, timeout=10)\n","    if result.returncode == 0:\n","        print(f\"âœ… Streamlit command works: {result.stdout.strip()}\")\n","    else:\n","        print(f\"âŒ Streamlit command failed: {result.stderr}\")\n","except Exception as e:\n","    print(f\"âŒ Streamlit command error: {e}\")\n","\n","print(\"\\nğŸ”‘ NGROK CHECK\")\n","print(\"=\"*50)\n","\n","try:\n","    from pyngrok import ngrok\n","    tunnels = ngrok.get_tunnels()\n","    print(f\"âœ… ngrok imported successfully\")\n","    print(f\"ğŸ“¡ Active tunnels: {len(tunnels)}\")\n","    for tunnel in tunnels:\n","        print(f\"ğŸŒ Tunnel: {tunnel}\")\n","except Exception as e:\n","    print(f\"âŒ ngrok error: {e}\")\n","\n","print(\"\\nğŸ“‹ RECOMMENDATIONS\")\n","print(\"=\"*50)\n","\n","if IN_COLAB:\n","    print(\"âœ… You're in Google Colab - Good!\")\n","    print(\"ğŸ’¡ Make sure you have T4 GPU enabled\")\n","    print(\"ğŸ’¡ Get your ngrok token from https://ngrok.com\")\n","else:\n","    print(\"âš ï¸  You're not in Google Colab\")\n","    print(\"ğŸ’¡ This setup is optimized for Google Colab\")\n","\n","print(\"\\nğŸš€ NEXT STEPS\")\n","print(\"=\"*50)\n","print(\"1. If all checks pass, run the fixed deployment script\")\n","print(\"2. Make sure your ngrok token is set correctly\")\n","print(\"3. Wait for the 'PLATFORM IS DEPLOYED' message\")\n","print(\"4. Click the provided public URL\")\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"ğŸ” DIAGNOSTIC COMPLETE\")\n","print(\"=\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lKfcjXpI-BV_","executionInfo":{"status":"ok","timestamp":1750585094671,"user_tz":-330,"elapsed":9441,"user":{"displayName":"korupolu sujana","userId":"11553913231016006169"}},"outputId":"f7462b25-fa9f-4cc0-94a2-77e5075f85b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” CITIZEN AI PLATFORM - DIAGNOSTIC CHECK\n","==================================================\n","ğŸ Python Version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n","âœ… Running in Google Colab\n","ğŸš€ PyTorch Version: 2.6.0+cu124\n","ğŸ¯ CUDA Available: True\n","ğŸ”¥ GPU Device: Tesla T4\n","ğŸ’¾ GPU Memory: 15.8 GB\n","\n","ğŸ”§ PACKAGE INSTALLATION CHECK\n","==================================================\n","âŒ streamlit - NOT FOUND\n","âœ… transformers\n","âœ… torch\n","âœ… plotly\n","âœ… textblob\n","âœ… wordcloud\n","âœ… matplotlib\n","âœ… seaborn\n","âœ… pandas\n","âœ… numpy\n","âœ… pyngrok\n","\n","ğŸ”„ Installing missing packages: ['streamlit']\n","âœ… Installed streamlit\n","\n","ğŸ“ FILE CHECK\n","==================================================\n","âœ… citizen_ai_app.py exists\n","ğŸ“ File size: 16354 bytes\n","âœ… File contains expected content\n","\n","ğŸŒ STREAMLIT CHECK\n","==================================================\n","âœ… Streamlit command works: Streamlit, version 1.46.0\n","\n","ğŸ”‘ NGROK CHECK\n","==================================================\n","âœ… ngrok imported successfully\n","ğŸ“¡ Active tunnels: 0\n","\n","ğŸ“‹ RECOMMENDATIONS\n","==================================================\n","âœ… You're in Google Colab - Good!\n","ğŸ’¡ Make sure you have T4 GPU enabled\n","ğŸ’¡ Get your ngrok token from https://ngrok.com\n","\n","ğŸš€ NEXT STEPS\n","==================================================\n","1. If all checks pass, run the fixed deployment script\n","2. Make sure your ngrok token is set correctly\n","3. Wait for the 'PLATFORM IS DEPLOYED' message\n","4. Click the provided public URL\n","\n","==================================================\n","ğŸ” DIAGNOSTIC COMPLETE\n","==================================================\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vEL1sq4fAIur"},"execution_count":null,"outputs":[]}]}